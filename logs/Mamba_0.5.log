>>> 'Pkg.add("Mamba")' log
INFO: Cloning cache of Mamba from https://github.com/brian-j-smith/Mamba.jl.git
INFO: Installing AutoHashEquals v0.1.2
INFO: Installing AxisAlgorithms v0.2.0
INFO: Installing BinDeps v0.6.0
INFO: Installing Blosc v0.3.0
INFO: Installing Cairo v0.3.1
INFO: Installing Calculus v0.2.2
INFO: Installing ColorTypes v0.5.2
INFO: Installing Colors v0.7.4
INFO: Installing Compose v0.5.3
INFO: Installing Contour v0.3.0
INFO: Installing CoupledFields v0.0.1
INFO: Installing DataArrays v0.4.1
INFO: Installing DataFrames v0.9.1
INFO: Installing DataStructures v0.6.1
INFO: Installing DiffBase v0.1.0
INFO: Installing Distances v0.4.1
INFO: Installing Distributions v0.13.0
INFO: Installing DualNumbers v0.3.0
INFO: Installing EzXML v0.4.5
INFO: Installing FileIO v0.5.1
INFO: Installing FixedPointNumbers v0.3.9
INFO: Installing ForwardDiff v0.4.2
INFO: Installing GZip v0.3.0
INFO: Installing Gadfly v0.6.3
INFO: Installing Graphics v0.2.0
INFO: Installing HDF5 v0.8.5
INFO: Installing Hexagons v0.1.0
INFO: Installing Hiccup v0.1.1
INFO: Installing Interpolations v0.6.3
INFO: Installing IterTools v0.1.0
INFO: Installing JLD v0.6.11
INFO: Installing Juno v0.2.7
INFO: Installing KernelDensity v0.3.2
INFO: Installing LegacyStrings v0.2.2
INFO: Installing LightGraphs v0.7.7
INFO: Installing LineSearches v0.1.5
INFO: Installing Loess v0.3.0
INFO: Installing MacroTools v0.3.7
INFO: Installing Mamba v0.10.1
INFO: Installing Measures v0.1.0
INFO: Installing Media v0.2.7
INFO: Installing NaNMath v0.2.6
INFO: Installing Optim v0.7.8
INFO: Installing PDMats v0.7.0
INFO: Installing ParserCombinator v1.7.11
INFO: Installing PositiveFactorizations v0.0.4
INFO: Installing QuadGK v0.1.3
INFO: Installing Ratios v0.1.0
INFO: Installing Reexport v0.0.3
INFO: Installing Rmath v0.2.0
INFO: Installing SHA v0.3.3
INFO: Installing ShowItLikeYouBuildIt v0.0.1
INFO: Installing Showoff v0.1.1
INFO: Installing SortingAlgorithms v0.1.1
INFO: Installing SpecialFunctions v0.2.0
INFO: Installing StaticArrays v0.3.1
INFO: Installing StatsBase v0.17.0
INFO: Installing StatsFuns v0.5.0
INFO: Installing URIParser v0.2.0
INFO: Installing WoodburyMatrices v0.2.2
INFO: Building Blosc
INFO: Building Cairo
INFO: Building Rmath
INFO: Building EzXML
INFO: Building HDF5
INFO: Package database updated

>>> 'Pkg.test("Mamba")' log
Julia Version 0.5.2
Commit f4c6c9d (2017-05-06 16:34 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-129-generic #178-Ubuntu SMP Fri Aug 11 12:48:20 UTC 2017 x86_64 x86_64
Memory: 2.9392738342285156 GB (1148.890625 MB free)
Uptime: 8016.0 sec
Load Avg:  1.046875  1.1689453125  1.0712890625
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3500 MHz     441011 s         86 s      37881 s     235884 s          6 s
#2  3500 MHz     138054 s         59 s      18063 s     629216 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-8-openjdk-amd64
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.5
2 required packages:
 - JSON                          0.13.0
 - Mamba                         0.10.1
60 additional packages:
 - AutoHashEquals                0.1.2
 - AxisAlgorithms                0.2.0
 - BinDeps                       0.6.0
 - Blosc                         0.3.0
 - Cairo                         0.3.1
 - Calculus                      0.2.2
 - ColorTypes                    0.5.2
 - Colors                        0.7.4
 - Compat                        0.31.0
 - Compose                       0.5.3
 - Contour                       0.3.0
 - CoupledFields                 0.0.1
 - DataArrays                    0.4.1
 - DataFrames                    0.9.1
 - DataStructures                0.6.1
 - DiffBase                      0.1.0
 - Distances                     0.4.1
 - Distributions                 0.13.0
 - DualNumbers                   0.3.0
 - EzXML                         0.4.5
 - FileIO                        0.5.1
 - FixedPointNumbers             0.3.9
 - ForwardDiff                   0.4.2
 - GZip                          0.3.0
 - Gadfly                        0.6.3
 - Graphics                      0.2.0
 - HDF5                          0.8.5
 - Hexagons                      0.1.0
 - Hiccup                        0.1.1
 - Interpolations                0.6.3
 - IterTools                     0.1.0
 - JLD                           0.6.11
 - Juno                          0.2.7
 - KernelDensity                 0.3.2
 - LegacyStrings                 0.2.2
 - LightGraphs                   0.7.7
 - LineSearches                  0.1.5
 - Loess                         0.3.0
 - MacroTools                    0.3.7
 - Measures                      0.1.0
 - Media                         0.2.7
 - NaNMath                       0.2.6
 - Optim                         0.7.8
 - PDMats                        0.7.0
 - ParserCombinator              1.7.11
 - PositiveFactorizations        0.0.4
 - QuadGK                        0.1.3
 - Ratios                        0.1.0
 - Reexport                      0.0.3
 - Rmath                         0.2.0
 - SHA                           0.3.3
 - ShowItLikeYouBuildIt          0.0.1
 - Showoff                       0.1.1
 - SortingAlgorithms             0.1.1
 - SpecialFunctions              0.2.0
 - StaticArrays                  0.3.1
 - StatsBase                     0.17.0
 - StatsFuns                     0.5.0
 - URIParser                     0.2.0
 - WoodburyMatrices              0.2.2
INFO: Testing Mamba
Running tests:

>>> Testing ../doc/tutorial/line.jl

WARNING: Method definition describe(AbstractArray) in module StatsBase at /home/vagrant/.julia/v0.5/StatsBase/src/scalarstats.jl:560 overwritten in module DataFrames at /home/vagrant/.julia/v0.5/DataFrames/src/abstractdataframe/abstractdataframe.jl:407.
digraph MambaModel {
	"y" [shape="ellipse", style="filled", fillcolor="gray85"];
	"mu" [shape="diamond", style="filled", fillcolor="gray85"];
		"mu" -> "y";
	"s2" [shape="ellipse"];
		"s2" -> "y";
	"beta" [shape="ellipse"];
		"beta" -> "mu";
	"xmat" [shape="box", style="filled", fillcolor="gray85"];
		"xmat" -> "mu";
}
MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:37:08 of 0:37:10 remaining]
Chain 1:  10% [0:00:35 of 0:00:39 remaining]
Chain 1:  20% [0:00:21 of 0:00:27 remaining]
Chain 1:  30% [0:00:15 of 0:00:22 remaining]
Chain 1:  40% [0:00:12 of 0:00:19 remaining]
Chain 1:  50% [0:00:09 of 0:00:17 remaining]
Chain 1:  60% [0:00:07 of 0:00:17 remaining]
Chain 1:  70% [0:00:05 of 0:00:17 remaining]
Chain 1:  80% [0:00:03 of 0:00:17 remaining]
Chain 1:  90% [0:00:02 of 0:00:17 remaining]
Chain 1: 100% [0:00:00 of 0:00:17 remaining]

Chain 2:   0% [0:00:21 of 0:00:21 remaining]
Chain 2:  10% [0:00:16 of 0:00:18 remaining]
Chain 2:  20% [0:00:14 of 0:00:18 remaining]
Chain 2:  30% [0:00:12 of 0:00:17 remaining]
Chain 2:  40% [0:00:10 of 0:00:16 remaining]
Chain 2:  50% [0:00:08 of 0:00:15 remaining]
Chain 2:  60% [0:00:06 of 0:00:15 remaining]
Chain 2:  70% [0:00:05 of 0:00:15 remaining]
Chain 2:  80% [0:00:03 of 0:00:15 remaining]
Chain 2:  90% [0:00:02 of 0:00:15 remaining]
Chain 2: 100% [0:00:00 of 0:00:16 remaining]

Chain 3:   0% [0:00:12 of 0:00:12 remaining]
Chain 3:  10% [0:00:13 of 0:00:14 remaining]
Chain 3:  20% [0:00:12 of 0:00:15 remaining]
Chain 3:  30% [0:00:10 of 0:00:15 remaining]
Chain 3:  40% [0:00:09 of 0:00:15 remaining]
Chain 3:  50% [0:00:08 of 0:00:15 remaining]
Chain 3:  60% [0:00:06 of 0:00:15 remaining]
Chain 3:  70% [0:00:05 of 0:00:16 remaining]
Chain 3:  80% [0:00:03 of 0:00:16 remaining]
Chain 3:  90% [0:00:02 of 0:00:16 remaining]
Chain 3: 100% [0:00:00 of 0:00:17 remaining]

MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:01:20 of 0:01:20 remaining]
Chain 1:  10% [0:00:24 of 0:00:27 remaining]
Chain 1:  20% [0:00:20 of 0:00:25 remaining]
Chain 1:  30% [0:00:17 of 0:00:25 remaining]
Chain 1:  40% [0:00:14 of 0:00:24 remaining]
Chain 1:  50% [0:00:11 of 0:00:22 remaining]
Chain 1:  60% [0:00:09 of 0:00:23 remaining]
Chain 1:  70% [0:00:07 of 0:00:22 remaining]
Chain 1:  80% [0:00:04 of 0:00:22 remaining]
Chain 1:  90% [0:00:02 of 0:00:23 remaining]
Chain 1: 100% [0:00:00 of 0:00:23 remaining]

Chain 2:   0% [0:00:20 of 0:00:20 remaining]
Chain 2:  10% [0:00:19 of 0:00:21 remaining]
Chain 2:  20% [0:00:13 of 0:00:16 remaining]
Chain 2:  30% [0:00:10 of 0:00:15 remaining]
Chain 2:  40% [0:00:09 of 0:00:15 remaining]
Chain 2:  50% [0:00:07 of 0:00:15 remaining]
Chain 2:  60% [0:00:06 of 0:00:15 remaining]
Chain 2:  70% [0:00:05 of 0:00:16 remaining]
Chain 2:  80% [0:00:03 of 0:00:16 remaining]
Chain 2:  90% [0:00:02 of 0:00:16 remaining]
Chain 2: 100% [0:00:00 of 0:00:15 remaining]

Chain 3:   0% [0:00:33 of 0:00:33 remaining]
Chain 3:  10% [0:00:24 of 0:00:26 remaining]
Chain 3:  20% [0:00:18 of 0:00:23 remaining]
Chain 3:  30% [0:00:17 of 0:00:25 remaining]
Chain 3:  40% [0:00:14 of 0:00:24 remaining]
Chain 3:  50% [0:00:13 of 0:00:25 remaining]
Chain 3:  60% [0:00:10 of 0:00:25 remaining]
Chain 3:  70% [0:00:07 of 0:00:25 remaining]
Chain 3:  80% [0:00:05 of 0:00:24 remaining]
Chain 3:  90% [0:00:02 of 0:00:24 remaining]
Chain 3: 100% [0:00:00 of 0:00:24 remaining]

MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:22:14 of 0:22:15 remaining]
Chain 1:  10% [0:00:14 of 0:00:15 remaining]
Chain 1:  20% [0:00:07 of 0:00:08 remaining]
Chain 1:  30% [0:00:04 of 0:00:06 remaining]
Chain 1:  40% [0:00:03 of 0:00:05 remaining]
Chain 1:  50% [0:00:02 of 0:00:04 remaining]
Chain 1:  60% [0:00:02 of 0:00:04 remaining]
Chain 1:  70% [0:00:01 of 0:00:04 remaining]
Chain 1:  80% [0:00:01 of 0:00:03 remaining]
Chain 1:  90% [0:00:00 of 0:00:03 remaining]
Chain 1: 100% [0:00:00 of 0:00:03 remaining]

Chain 2:   0% [0:00:02 of 0:00:02 remaining]
Chain 2:  10% [0:00:01 of 0:00:01 remaining]
Chain 2:  20% [0:00:01 of 0:00:02 remaining]
Chain 2:  30% [0:00:01 of 0:00:02 remaining]
Chain 2:  40% [0:00:01 of 0:00:02 remaining]
Chain 2:  50% [0:00:01 of 0:00:02 remaining]
Chain 2:  60% [0:00:01 of 0:00:02 remaining]
Chain 2:  70% [0:00:01 of 0:00:02 remaining]
Chain 2:  80% [0:00:00 of 0:00:02 remaining]
Chain 2:  90% [0:00:00 of 0:00:02 remaining]
Chain 2: 100% [0:00:00 of 0:00:02 remaining]

Chain 3:   0% [0:00:02 of 0:00:02 remaining]
Chain 3:  10% [0:00:02 of 0:00:02 remaining]
Chain 3:  20% [0:00:01 of 0:00:02 remaining]
Chain 3:  30% [0:00:01 of 0:00:02 remaining]
Chain 3:  40% [0:00:01 of 0:00:02 remaining]
Chain 3:  50% [0:00:01 of 0:00:02 remaining]
Chain 3:  60% [0:00:01 of 0:00:02 remaining]
Chain 3:  70% [0:00:01 of 0:00:02 remaining]
Chain 3:  80% [0:00:00 of 0:00:02 remaining]
Chain 3:  90% [0:00:00 of 0:00:02 remaining]
Chain 3: 100% [0:00:00 of 0:00:02 remaining]

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Gelman, Rubin, and Brooks Diagnostic:
              PSRF 97.5%
     beta[1] 1.002 1.003
     beta[2] 1.003 1.003
          s2 1.002 1.003
Multivariate 1.000   NaN

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Geweke Diagnostic:
First Window Fraction = 0.1
Second Window Fraction = 0.5

        Z-score p-value
beta[1]  -0.450  0.6528
beta[2]   0.657  0.5112
     s2  -2.448  0.0144

        Z-score p-value
beta[1]   0.266  0.7904
beta[2]  -0.291  0.7714
     s2  -1.361  0.1736

        Z-score p-value
beta[1]  -0.022  0.9821
beta[2]   0.283  0.7771
     s2  -1.677  0.0936

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Heidelberger and Welch Diagnostic:
Target Halfwidth Ratio = 0.1
Alpha = 0.05

        Burn-in Stationarity p-value    Mean     Halfwidth  Test
beta[1]     251            1  0.6081 0.56689198 0.062442773    0
beta[2]     251            1  0.6189 0.80665808 0.016980313    1
     s2     251            1  0.5998 1.15916075 0.176484028    0

        Burn-in Stationarity p-value    Mean     Halfwidth  Test
beta[1]     251            1  0.5622 0.57205135 0.057180269    1
beta[2]     251            1  0.4376 0.80577830 0.015814972    1
     s2     251            1  0.2885 1.13947629 0.155614707    0

        Burn-in Stationarity p-value    Mean     Halfwidth  Test
beta[1]     251            1  0.1861 0.54408853 0.064555202    0
beta[2]     251            1  0.1812 0.81327110 0.018693595    1
     s2     251            1  0.4505 1.29747773 0.265610771    0

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Raftery and Lewis Diagnostic:
Quantile (q) = 0.025
Accuracy (r) = 0.005
Probability (s) = 0.95

        Thinning Burn-in    Total   Nmin Dependence Factor
beta[1]        2     273 2.3015×10⁴ 3746         6.1438868
beta[2]        2     265 1.6555×10⁴ 3746         4.4193807
     s2        2     257 8.6890×10³ 3746         2.3195408

        Thinning Burn-in    Total   Nmin Dependence Factor
beta[1]        2     271 2.2509×10⁴ 3746         6.0088094
beta[2]        2     265 1.4793×10⁴ 3746         3.9490123
     s2        2     257 8.4090×10³ 3746         2.2447944

        Thinning Burn-in    Total   Nmin Dependence Factor
beta[1]        2     275 2.5215×10⁴ 3746         6.7311799
beta[2]        2     265 1.5069×10⁴ 3746         4.0226909
     s2        2     255 7.8770×10³ 3746         2.1027763

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Empirical Posterior Estimates:
           Mean        SD       Naive SE      MCSE        ESS  
beta[1] 0.56101062 1.15536100 0.009553660 0.0156473377 4875.000
beta[2] 0.80856916 0.34726763 0.002871550 0.0043911549 4875.000
     s2 1.19870492 1.56864002 0.012971057 0.0600219481  683.009

Quantiles:
            2.5%        25.0%       50.0%      75.0%     97.5%  
beta[1] -1.80132114 -0.055811678 0.56674250 1.19351175 2.8785273
beta[2]  0.13335967  0.623366952 0.80857307 0.99002252 1.5021781
     s2  0.17591020  0.391720912 0.67219297 1.29812099 5.8893323

         95% Lower  95% Upper
beta[1] -1.82356032 2.8269553
beta[2]  0.13379775 1.5023687
     s2  0.08105333 4.0471892

          beta[1]      beta[2]        s2     
beta[1]  1.00000000 -0.902530376 -0.058490849
beta[2] -0.90253038  1.000000000  0.031009024
     s2 -0.05849085  0.031009024  1.000000000

           Lag 2       Lag 10        Lag 20       Lag 100   
beta[1] 0.32363233   -0.06878183  -0.010020497  -0.004079708
beta[2] 0.25761327   -0.06478366  -0.025873051   0.012010870
     s2 0.76778414    0.35282562   0.131592764   0.044746958

           Lag 2       Lag 10        Lag 20       Lag 100   
beta[1] 0.28503104  0.0148294471   0.005551656   0.019832008
beta[2] 0.22748197 -0.0040102710   0.008956909   0.028501515
     s2 0.72262408  0.2915667603   0.132101886  -0.037133178

           Lag 2       Lag 10        Lag 20       Lag 100   
beta[1] 0.33115909  -0.007933394  0.0195832385 -0.0064098384
beta[2] 0.28378709   0.004359849 -0.0022212270 -0.0148778720
     s2 0.85240357   0.545525422  0.2687135047  0.0165931570

             Change Rate
     beta[1]       0.876
     beta[2]       0.876
          s2       1.000
Multivariate       1.000

MCMC Processing of 4875 Iterations x 3 Chains...

Chain 1:   0% [0:02:20 of 0:02:20 remaining]
Chain 1:  10% [0:00:03 of 0:00:03 remaining]
Chain 1:  20% [0:00:01 of 0:00:02 remaining]
Chain 1:  30% [0:00:01 of 0:00:01 remaining]
Chain 1:  40% [0:00:01 of 0:00:01 remaining]
Chain 1:  50% [0:00:00 of 0:00:01 remaining]
Chain 1:  60% [0:00:00 of 0:00:01 remaining]
Chain 1:  70% [0:00:00 of 0:00:01 remaining]
Chain 1:  80% [0:00:00 of 0:00:01 remaining]
Chain 1:  90% [0:00:00 of 0:00:01 remaining]
Chain 1: 100% [0:00:00 of 0:00:01 remaining]

Chain 2:   0% [0:04:28 of 0:04:28 remaining]
Chain 2:  10% [0:00:05 of 0:00:06 remaining]
Chain 2:  20% [0:00:02 of 0:00:03 remaining]
Chain 2:  30% [0:00:01 of 0:00:02 remaining]
Chain 2:  40% [0:00:01 of 0:00:02 remaining]
Chain 2:  50% [0:00:01 of 0:00:01 remaining]
Chain 2:  60% [0:00:00 of 0:00:01 remaining]
Chain 2:  70% [0:00:00 of 0:00:01 remaining]
Chain 2:  80% [0:00:00 of 0:00:01 remaining]
Chain 2:  90% [0:00:00 of 0:00:01 remaining]
Chain 2: 100% [0:00:00 of 0:00:01 remaining]

Chain 3:   0% [0:06:22 of 0:06:23 remaining]
Chain 3:  10% [0:00:07 of 0:00:08 remaining]
Chain 3:  20% [0:00:03 of 0:00:04 remaining]
Chain 3:  30% [0:00:02 of 0:00:03 remaining]
Chain 3:  40% [0:00:01 of 0:00:02 remaining]
Chain 3:  50% [0:00:01 of 0:00:02 remaining]
Chain 3:  60% [0:00:01 of 0:00:02 remaining]
Chain 3:  70% [0:00:00 of 0:00:01 remaining]
Chain 3:  80% [0:00:00 of 0:00:01 remaining]
Chain 3:  90% [0:00:00 of 0:00:01 remaining]
Chain 3: 100% [0:00:00 of 0:00:01 remaining]

      DIC    Effective Parameters
pD 14.194649            1.3814665
pV 22.608880            5.5885820

Iterations = 1000:5000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 2001

Empirical Posterior Estimates:
           Mean        SD       Naive SE       MCSE       ESS   
beta[1] 0.53682913 1.10492601 0.0142609687 0.025219002 1919.5994
beta[2] 0.81491586 0.33253903 0.0042919876 0.007129130 2001.0000

Quantiles:
            2.5%        25.0%       50.0%      75.0%     97.5%  
beta[1] -1.64673860 -0.059486655 0.55755745 1.15954184 2.6330071
beta[2]  0.19097135  0.634450846 0.81168506 0.98640910 1.4749587

MCMC Simulation of 5000 Iterations x 3 Chains...

Chain 1:   0% [0:00:05 of 0:00:05 remaining]
Chain 1:  10% [0:00:07 of 0:00:08 remaining]
Chain 1:  20% [0:00:06 of 0:00:08 remaining]
Chain 1:  30% [0:00:06 of 0:00:08 remaining]
Chain 1:  40% [0:00:05 of 0:00:09 remaining]
Chain 1:  50% [0:00:04 of 0:00:09 remaining]
Chain 1:  60% [0:00:04 of 0:00:09 remaining]
Chain 1:  70% [0:00:03 of 0:00:09 remaining]
Chain 1:  80% [0:00:02 of 0:00:09 remaining]
Chain 1:  90% [0:00:01 of 0:00:09 remaining]
Chain 1: 100% [0:00:00 of 0:00:09 remaining]

Chain 2:   0% [0:00:07 of 0:00:07 remaining]
Chain 2:  10% [0:00:08 of 0:00:09 remaining]
Chain 2:  20% [0:00:08 of 0:00:10 remaining]
Chain 2:  30% [0:00:07 of 0:00:10 remaining]
Chain 2:  40% [0:00:06 of 0:00:10 remaining]
Chain 2:  50% [0:00:05 of 0:00:11 remaining]
Chain 2:  60% [0:00:04 of 0:00:11 remaining]
Chain 2:  70% [0:00:03 of 0:00:11 remaining]
Chain 2:  80% [0:00:02 of 0:00:10 remaining]
Chain 2:  90% [0:00:01 of 0:00:10 remaining]
Chain 2: 100% [0:00:00 of 0:00:10 remaining]

Chain 3:   0% [0:00:08 of 0:00:08 remaining]
Chain 3:  10% [0:00:06 of 0:00:06 remaining]
Chain 3:  20% [0:00:06 of 0:00:07 remaining]
Chain 3:  30% [0:00:06 of 0:00:08 remaining]
Chain 3:  40% [0:00:05 of 0:00:08 remaining]
Chain 3:  50% [0:00:04 of 0:00:08 remaining]
Chain 3:  60% [0:00:03 of 0:00:08 remaining]
Chain 3:  70% [0:00:02 of 0:00:08 remaining]
Chain 3:  80% [0:00:02 of 0:00:08 remaining]
Chain 3:  90% [0:00:01 of 0:00:08 remaining]
Chain 3: 100% [0:00:00 of 0:00:09 remaining]

Iterations = 252:15000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 7375

Empirical Posterior Estimates:
           Mean        SD       Naive SE       MCSE       ESS  
beta[1] 0.57907374 1.19140410 0.0080097222 0.0131076784 7375.00
beta[2] 0.80525477 0.35875052 0.0024118534 0.0035882927 7375.00
     s2 1.28349978 1.85295643 0.0124572898 0.0664543463  777.47

Quantiles:
            2.5%        25.0%       50.0%      75.0%     97.5%  
beta[1] -1.87899349 -0.035260472 0.57863180 1.20264869 2.9593208
beta[2]  0.09482447  0.622327869 0.80640777 0.98669446 1.5361072
     s2  0.17371212  0.396874135 0.68123636 1.32994539 6.8345809

Object of type "Mamba.Model"
-------------------------------------------------------------------------------
y:
An unmonitored node of type "5-element Mamba.ArrayStochastic{1}"
[1.0,3.0,3.0,3.0,5.0]

Distribution:
IsoNormal(
dim: 5
μ: [3.23845,5.28663,7.33481,9.38299,11.4312]
Σ: [1.63439 0.0 0.0 0.0 0.0; 0.0 1.63439 0.0 0.0 0.0; 0.0 0.0 1.63439 0.0 0.0; 0.0 0.0 0.0 1.63439 0.0; 0.0 0.0 0.0 0.0 1.63439]
)

Function:
LambdaInfo for (::Mamba.##298#299)(::Mamba.Model)
:(begin 
        f = #5
        SSAValue(1) = $(Expr(:invoke, LambdaInfo for getindex(::Mamba.Model, ::Symbol), :(Mamba.getindex), :(model), :(:mu)))
        SSAValue(0) = $(Expr(:invoke, LambdaInfo for getindex(::Mamba.Model, ::Symbol), :(Mamba.getindex), :(model), :(:s2)))
        return (Main.MvNormal)(SSAValue(1),(Main.sqrt)(SSAValue(0)))
    end)

Source Nodes:
Symbol[:mu,:s2]

Target Nodes:
Symbol[]
-------------------------------------------------------------------------------
s2:
A monitored node of type "Mamba.ScalarStochastic"
1.634388973402134

Distribution:
Distributions.InverseGamma{Float64}(
invd: Distributions.Gamma{Float64}(α=0.001, θ=1000.0)
θ: 0.001
)

Function:
LambdaInfo for (::Mamba.##304#305)(::Mamba.Model)
:(begin 
        f = #8
        return $(Expr(:invoke, LambdaInfo for Distributions.InverseGamma{Float64}(::Float64, ::Float64), Distributions.InverseGamma{Float64}, 0.001, 0.001))
    end::Distributions.InverseGamma{Float64})

Source Nodes:
Symbol[]

Target Nodes:
Symbol[:y]
-------------------------------------------------------------------------------
xmat:
[1.0 1.0; 1.0 2.0; 1.0 3.0; 1.0 4.0; 1.0 5.0]
-------------------------------------------------------------------------------
beta:
A monitored node of type "2-element Mamba.ArrayStochastic{1}"
[1.19027,2.04818]

Distribution:
ZeroMeanIsoNormal(
dim: 2
μ: [0.0,0.0]
Σ: [1000.0 0.0; 0.0 1000.0]
)

Function:
LambdaInfo for (::Mamba.##302#303)(::Mamba.Model)
:(begin 
        f = #7
        return $(Expr(:invoke, LambdaInfo for (::##7#11)(), :(f)))
    end::Distributions.MvNormal{Float64,PDMats.ScalMat{Float64},Distributions.ZeroVector{Float64}})

Source Nodes:
Symbol[]

Target Nodes:
Symbol[:mu,:y]
-------------------------------------------------------------------------------
mu:
An unmonitored node of type "5-element Mamba.ArrayLogical{1}"
[3.23845,5.28663,7.33481,9.38299,11.4312]
Function:
LambdaInfo for (::Mamba.##300#301)(::Mamba.Model)
:(begin 
        f = #6
        SSAValue(1) = $(Expr(:invoke, LambdaInfo for getindex(::Mamba.Model, ::Symbol), :(Mamba.getindex), :(model), :(:xmat)))
        SSAValue(0) = $(Expr(:invoke, LambdaInfo for getindex(::Mamba.Model, ::Symbol), :(Mamba.getindex), :(model), :(:beta)))
        return SSAValue(1) * SSAValue(0)
    end)

Source Nodes:
Symbol[:xmat,:beta]

Target Nodes:
Symbol[:y]

>>> Testing ../doc/samplers/amm.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE       MCSE       ESS   
b0 0.67372957 1.07776604 0.0152419136 0.064652461 277.89381
b1 0.77112512 0.32135034 0.0045445801 0.017982404 319.34640
s2 1.29658296 2.18979550 0.0309683849 0.139974432 244.74267

Quantiles:
       2.5%        25.0%       50.0%      75.0%     97.5%  
b0 -1.451022943 0.069490369 0.65978541 1.19631191 3.1685322
b1  0.050258268 0.611291896 0.79094107 0.94652458 1.4132212
s2  0.174653274 0.389575233 0.65132492 1.31853897 6.8337311


>>> Testing ../doc/samplers/amwg.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean       SD      Naive SE       MCSE        ESS   
b0 0.5983881 1.5898262 0.0224835377 0.162560986  95.645964
b1 0.8006155 0.4358909 0.0061644284 0.042038394 107.513597
s2 2.2710644 9.1262030 0.1290640007 0.686946293 176.495925

Quantiles:
       2.5%       25.0%      50.0%      75.0%      97.5%  
b0 -2.17471713 0.03038741 0.67912808 1.30557650  3.3496859
b1  0.03755749 0.60096712 0.77599921 0.96011511  1.7513752
s2  0.16901125 0.39945048 0.70340929 1.47274298 13.3376638


>>> Testing ../doc/samplers/bhmc.jl

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean      SD       Naive SE       MCSE         ESS   
 gamma[1] 0.5425 0.49821539 0.0049821539 0.0043794170 10000.0000
 gamma[2] 1.0000 0.00000000 0.0000000000 0.0000000000 10000.0000
 gamma[3] 0.3125 0.46353558 0.0046353558 0.0052442850  7812.5639
 gamma[4] 1.0000 0.00000000 0.0000000000 0.0000000000 10000.0000
 gamma[5] 0.7132 0.45228997 0.0045228997 0.0056726884  6357.0562
 gamma[6] 0.7342 0.44178035 0.0044178035 0.0055526497  6330.1242
 gamma[7] 1.0000 0.00000000 0.0000000000 0.0000000000 10000.0000
 gamma[8] 0.5202 0.49961677 0.0049961677 0.0030648875 10000.0000
 gamma[9] 0.4972 0.50001716 0.0050001716 0.0063230858  6253.3346
gamma[10] 0.7768 0.41641218 0.0041641218 0.0044537579  8741.6542

Quantiles:
          2.5% 25.0% 50.0% 75.0% 97.5%
 gamma[1]    0     0     1     1     1
 gamma[2]    1     1     1     1     1
 gamma[3]    0     0     0     1     1
 gamma[4]    1     1     1     1     1
 gamma[5]    0     0     1     1     1
 gamma[6]    0     0     1     1     1
 gamma[7]    1     1     1     1     1
 gamma[8]    0     0     1     1     1
 gamma[9]    0     0     0     1     1
gamma[10]    0     1     1     1     1


>>> Testing ../doc/samplers/bia.jl

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean       SD        Naive SE       MCSE         ESS    
 gamma[1] 0.6803 0.466383599 0.00466383599 0.0130286824  1281.40243
 gamma[2] 0.9996 0.019996999 0.00019996999 0.0004000000  2499.24992
 gamma[3] 0.0012 0.034621956 0.00034621956 0.0009458041  1339.98766
 gamma[4] 0.9973 0.051893924 0.00051893924 0.0027000000   369.40731
 gamma[5] 0.9978 0.046854877 0.00046854877 0.0022000000   453.59081
 gamma[6] 0.9997 0.017318776 0.00017318776 0.0003000000  3332.66660
 gamma[7] 0.9982 0.042390325 0.00042390325 0.0018000000   554.61102
 gamma[8] 0.6932 0.461188714 0.00461188714 0.0113109226  1662.49857
 gamma[9] 0.0000 0.000000000 0.00000000000 0.0000000000 10000.00000
gamma[10] 0.9986 0.037392243 0.00037392243 0.0014000000   713.35705

Quantiles:
          2.5% 25.0% 50.0% 75.0% 97.5%
 gamma[1]    0     0     1     1     1
 gamma[2]    1     1     1     1     1
 gamma[3]    0     0     0     0     0
 gamma[4]    1     1     1     1     1
 gamma[5]    1     1     1     1     1
 gamma[6]    1     1     1     1     1
 gamma[7]    1     1     1     1     1
 gamma[8]    0     0     1     1     1
 gamma[9]    0     0     0     0     0
gamma[10]    1     1     1     1     1


>>> Testing ../doc/samplers/bmc3.jl

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean       SD        Naive SE       MCSE          ESS    
 gamma[1] 0.6859 0.464179638 0.00464179638 0.0159190471   850.233153
 gamma[2] 0.9992 0.028274369 0.00028274369 0.0008000000  1249.124912
 gamma[3] 0.0125 0.111107986 0.00111107986 0.0073887370   226.125809
 gamma[4] 0.9988 0.034621956 0.00034621956 0.0012000000   832.416575
 gamma[5] 0.9570 0.202867236 0.00202867236 0.0194803709   108.449957
 gamma[6] 0.9997 0.017318776 0.00017318776 0.0002227015  6047.669940
 gamma[7] 1.0000 0.000000000 0.00000000000 0.0000000000 10000.000000
 gamma[8] 0.6883 0.463211147 0.00463211147 0.0185983897   620.307903
 gamma[9] 0.0440 0.205105355 0.00205105355 0.0196884830   108.524924
gamma[10] 0.9635 0.187540041 0.00187540041 0.0178083938   110.901780

Quantiles:
          2.5% 25.0% 50.0% 75.0% 97.5%
 gamma[1]    0     0     1     1     1
 gamma[2]    1     1     1     1     1
 gamma[3]    0     0     0     0     0
 gamma[4]    1     1     1     1     1
 gamma[5]    0     1     1     1     1
 gamma[6]    1     1     1     1     1
 gamma[7]    1     1     1     1     1
 gamma[8]    0     0     1     1     1
 gamma[9]    0     0     0     0     1
gamma[10]    0     1     1     1     1

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean       SD        Naive SE       MCSE        ESS    
 gamma[1] 0.6770 0.467646094 0.00467646094 0.016071494   846.68547
 gamma[2] 0.9993 0.026449574 0.00026449574 0.000700000  1427.71420
 gamma[3] 0.0033 0.057353631 0.00057353631 0.003300000   302.06051
 gamma[4] 0.9991 0.029987996 0.00029987996 0.000900000  1110.22213
 gamma[5] 0.9982 0.042390325 0.00042390325 0.001800000   554.61102
 gamma[6] 0.9994 0.024488772 0.00024488772 0.000600000  1665.83325
 gamma[7] 0.9997 0.017318776 0.00017318776 0.000300000  3332.66660
 gamma[8] 0.6785 0.467075546 0.00467075546 0.015762361   878.07523
 gamma[9] 0.0000 0.000000000 0.00000000000 0.000000000 10000.00000
gamma[10] 0.9999 0.010000000 0.00010000000 0.000100000 10000.00000

Quantiles:
          2.5% 25.0% 50.0% 75.0% 97.5%
 gamma[1]    0     0     1     1     1
 gamma[2]    1     1     1     1     1
 gamma[3]    0     0     0     0     0
 gamma[4]    1     1     1     1     1
 gamma[5]    1     1     1     1     1
 gamma[6]    1     1     1     1     1
 gamma[7]    1     1     1     1     1
 gamma[8]    0     0     1     1     1
 gamma[9]    0     0     0     0     0
gamma[10]    1     1     1     1     1


>>> Testing ../doc/samplers/bmg.jl

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean       SD        Naive SE       MCSE        ESS    
 gamma[1] 0.6426 0.479257977 0.00479257977 0.019055273   632.56964
 gamma[2] 0.9974 0.050926411 0.00050926411 0.002600000   383.65375
 gamma[3] 0.0014 0.037392243 0.00037392243 0.001400000   713.35705
 gamma[4] 0.9977 0.047905527 0.00047905527 0.002300000   433.82599
 gamma[5] 0.9961 0.062331200 0.00062331200 0.003900000   255.43580
 gamma[6] 0.9996 0.019996999 0.00019996999 0.000400000  2499.24992
 gamma[7] 1.0000 0.000000000 0.00000000000 0.000000000 10000.00000
 gamma[8] 0.7135 0.452148420 0.00452148420 0.022320157   410.36240
 gamma[9] 0.0005 0.022356207 0.00022356207 0.000500000  1999.19992
gamma[10] 0.9964 0.059894897 0.00059894897 0.003600000   276.80546

Quantiles:
          2.5% 25.0% 50.0% 75.0% 97.5%
 gamma[1]    0     0     1     1     1
 gamma[2]    1     1     1     1     1
 gamma[3]    0     0     0     0     0
 gamma[4]    1     1     1     1     1
 gamma[5]    1     1     1     1     1
 gamma[6]    1     1     1     1     1
 gamma[7]    1     1     1     1     1
 gamma[8]    0     0     1     1     1
 gamma[9]    0     0     0     0     0
gamma[10]    1     1     1     1     1

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean       SD        Naive SE       MCSE        ESS    
 gamma[1] 0.6608 0.473461484 0.00473461484 0.021104684   503.28225
 gamma[2] 0.9973 0.051893924 0.00051893924 0.002700000   369.40731
 gamma[3] 0.0030 0.054692770 0.00054692770 0.003000000   332.36657
 gamma[4] 1.0000 0.000000000 0.00000000000 0.000000000 10000.00000
 gamma[5] 0.9959 0.063903039 0.00063903039 0.004100000   242.92673
 gamma[6] 0.9970 0.054692770 0.00054692770 0.003000000   332.36657
 gamma[7] 0.9996 0.019996999 0.00019996999 0.000400000  2499.24992
 gamma[8] 0.6948 0.460515111 0.00460515111 0.017472911   694.63591
 gamma[9] 0.0000 0.000000000 0.00000000000 0.000000000 10000.00000
gamma[10] 0.9990 0.031608542 0.00031608542 0.001000000   999.09991

Quantiles:
          2.5% 25.0% 50.0% 75.0% 97.5%
 gamma[1]    0     0     1     1     1
 gamma[2]    1     1     1     1     1
 gamma[3]    0     0     0     0     0
 gamma[4]    1     1     1     1     1
 gamma[5]    1     1     1     1     1
 gamma[6]    1     1     1     1     1
 gamma[7]    1     1     1     1     1
 gamma[8]    0     0     1     1     1
 gamma[9]    0     0     0     0     0
gamma[10]    1     1     1     1     1


>>> Testing ../doc/samplers/hmc.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE       MCSE       ESS   
b0 0.5772432  1.18402293 0.0167446128 0.027883682 1803.1006
b1 0.8022538  0.37671405 0.0053275412 0.008170591 2125.7719
s2 1.4971811 11.28460781 0.1595884542 0.195664541 3326.2025

Quantiles:
       2.5%        25.0%       50.0%      75.0%     97.5%  
b0 -1.714610896 0.015592733 0.55534270 1.17186976 3.0307402
b1  0.092367742 0.631187676 0.80982814 0.97620155 1.4874951
s2  0.178874019 0.383680766 0.66198223 1.22565452 7.0403625

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE       MCSE       ESS   
b0 0.59496699 1.37677793 0.0194705802 0.035517823 1502.5714
b1 0.79853916 0.41130398 0.0058167167 0.009917444 1719.9916
s2 1.80970811 8.36078976 0.1182394227 0.312469964  715.9423

Quantiles:
       2.5%        25.0%      50.0%     75.0%     97.5%  
b0 -1.89540401 -0.028843232 0.6365522 1.2257351 3.0797518
b1  0.06653081  0.605516575 0.7862590 0.9763511 1.5777984
s2  0.16937961  0.390805781 0.6795564 1.3775763 8.2227653


>>> Testing ../doc/samplers/mala.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE      MCSE        ESS   
b0 0.63913008 0.96456516 0.013641011 0.109737267  77.260028
b1 0.79071667 0.29199225 0.004129394 0.030293853  92.903834
s2 1.16990867 2.21952343 0.031388801 0.159528112 193.573104

Quantiles:
       2.5%       25.0%       50.0%     75.0%     97.5%  
b0 -1.11636066 0.021295738 0.56942794 1.1958553 2.8185967
b1  0.13351555 0.622505238 0.81840154 0.9631912 1.3448359
s2  0.15082458 0.428174945 0.70535341 1.1853768 4.7815748

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean         SD       Naive SE      MCSE       ESS   
b0 0.61457782  1.09976885 0.0155530802 0.12576800  76.46497
b1 0.73181199  0.40025128 0.0056604079 0.04444227  81.10973
s2 2.93138741 13.82161613 0.1954671698 1.21062808 130.34554

Quantiles:
       2.5%       25.0%     50.0%     75.0%      97.5%  
b0 -1.29782609 0.00000000 0.4671962 1.0542319  3.6409243
b1 -0.09214223 0.57229743 0.7547751 0.9690808  1.4525190
s2  0.23962808 0.44855541 0.8566210 1.6689977 13.7557720


>>> Testing ../doc/samplers/nuts.jl

Iterations = 1001:5000
Thinning interval = 1
Chains = 1
Samples per chain = 4000

Empirical Posterior Estimates:
      Mean        SD       Naive SE      MCSE       ESS   
b0 0.59242566 1.12664340 0.0178137962 0.07262530 240.65621
b1 0.79955933 0.34124494 0.0053955563 0.02021753 284.88937
s2 1.23041170 2.28573689 0.0361406736 0.10397815 483.24600

Quantiles:
       2.5%        25.0%       50.0%     75.0%     97.5%  
b0 -1.60156914 -0.018689413 0.55936277 1.1737069 2.9302371
b1  0.09581153  0.620652115 0.81772866 0.9840868 1.4803526
s2  0.20677131  0.401460055 0.67740807 1.2526098 5.6494793


>>> Testing ../doc/samplers/rwm.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE       MCSE       ESS   
b0 0.50814446 0.95649687 0.0135269085 0.105741241  81.82355
b1 0.82845138 0.29152116 0.0041227318 0.031171455  87.46333
s2 1.09352799 1.51797233 0.0214673706 0.072343598 440.27852

Quantiles:
       2.5%       25.0%       50.0%     75.0%     97.5%  
b0 -1.40016641 -0.10053289 0.52105167 1.1664813 2.3420987
b1  0.27991615  0.63683768 0.81974517 1.0071005 1.4073847
s2  0.18553138  0.40129932 0.68770085 1.1990384 4.5481870


>>> Testing ../doc/samplers/slice.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE     MCSE        ESS   
b0 0.95121197 1.39800189 0.019770732 0.16058158  75.792124
b1 0.70804449 0.38972473 0.005511540 0.04083353  91.092384
s2 1.68897644 4.98568184 0.070508189 0.41400577 145.022818

Quantiles:
       2.5%       25.0%      50.0%     75.0%      97.5%  
b0 -1.18134932 0.22505842 0.75133123 1.3961206  5.2902465
b1 -0.24667682 0.57032710 0.75324890 0.9190770  1.3468726
s2  0.16329786 0.38360098 0.64859723 1.3052359 10.1810850

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean       SD       Naive SE       MCSE       ESS   
b0 0.5918173 0.97247832 0.0137529202 0.088171432 121.64775
b1 0.8010611 0.29639544 0.0041916646 0.023762408 155.58300
s2 1.1918742 2.91584961 0.0412363406 0.118877515 601.63183

Quantiles:
       2.5%       25.0%      50.0%      75.0%     97.5%  
b0 -1.53289453 0.06956086 0.62086125 1.12897029 2.5922503
b1  0.19746079 0.64147194 0.79976963 0.95676223 1.4487573
s2  0.16453963 0.36687076 0.61831108 1.15173238 5.9535381


>>> Testing ../doc/samplers/slicesimplex.jl

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean         SD        Naive SE        MCSE        ESS   
rho[1] 0.239541518 0.044313241 0.00044313241 0.00137952343 1031.8317
rho[2] 0.399631046 0.051537720 0.00051537720 0.00160991471 1024.8131
rho[3] 0.238853960 0.044444932 0.00044444932 0.00138650632 1027.5448
rho[4] 0.103934653 0.031431644 0.00031431644 0.00080360557 1529.8481
rho[5] 0.018038823 0.014320007 0.00014320007 0.00025593598 3130.5724

Quantiles:
           2.5%        25.0%       50.0%       75.0%       97.5%  
rho[1] 0.1589529600 0.208389355 0.237188013 0.268762197 0.33381195
rho[2] 0.2962031414 0.364947507 0.399943947 0.434375282 0.50173322
rho[3] 0.1583092404 0.207145744 0.236661105 0.268110999 0.33276747
rho[4] 0.0515478833 0.081696193 0.100659153 0.123514195 0.17268427
rho[5] 0.0017654595 0.007907924 0.014502272 0.024265619 0.05344365


>>> Testing ../doc/mcmc/readcoda.jl

Iterations = 1:200
Thinning interval = 1
Chains = 1,2
Samples per chain = 200

Empirical Posterior Estimates:
         Mean       SD       Naive SE      MCSE       ESS   
alpha 3.0025394 0.53475753 0.026737877 0.018902157 200.00000
 beta 0.8013086 0.39267477 0.019633739 0.030895834 161.53482
sigma 1.0821777 0.94869150 0.047434575 0.061191837 200.00000

Quantiles:
         2.5%      25.0%   50.0%   75.0%     97.5%  
alpha  1.8322543 2.751095 3.0257 3.2709700 3.9511365
 beta -0.0125375 0.599750 0.8065 1.0079525 1.5292802
sigma  0.4329000 0.625000 0.8360 1.2378125 2.8597185


>>> Testing ../doc/mcmc/newunivardist.jl

MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:00:32 of 0:00:32 remaining]
Chain 1:  10% [0:00:06 of 0:00:07 remaining]
Chain 1:  20% [0:00:05 of 0:00:06 remaining]
Chain 1:  30% [0:00:04 of 0:00:05 remaining]
Chain 1:  40% [0:00:03 of 0:00:06 remaining]
Chain 1:  50% [0:00:03 of 0:00:06 remaining]
Chain 1:  60% [0:00:02 of 0:00:06 remaining]
Chain 1:  70% [0:00:02 of 0:00:06 remaining]
Chain 1:  80% [0:00:01 of 0:00:06 remaining]
Chain 1:  90% [0:00:01 of 0:00:06 remaining]
Chain 1: 100% [0:00:00 of 0:00:06 remaining]

Chain 2:   0% [0:00:12 of 0:00:12 remaining]
Chain 2:  10% [0:00:12 of 0:00:13 remaining]
Chain 2:  20% [0:00:10 of 0:00:12 remaining]
Chain 2:  30% [0:00:08 of 0:00:12 remaining]
Chain 2:  40% [0:00:06 of 0:00:11 remaining]
Chain 2:  50% [0:00:06 of 0:00:11 remaining]
Chain 2:  60% [0:00:05 of 0:00:12 remaining]
Chain 2:  70% [0:00:04 of 0:00:12 remaining]
Chain 2:  80% [0:00:02 of 0:00:12 remaining]
Chain 2:  90% [0:00:01 of 0:00:11 remaining]
Chain 2: 100% [0:00:00 of 0:00:11 remaining]

Chain 3:   0% [0:00:08 of 0:00:08 remaining]
Chain 3:  10% [0:00:06 of 0:00:06 remaining]
Chain 3:  20% [0:00:06 of 0:00:08 remaining]
Chain 3:  30% [0:00:05 of 0:00:08 remaining]
Chain 3:  40% [0:00:05 of 0:00:08 remaining]
Chain 3:  50% [0:00:04 of 0:00:08 remaining]
Chain 3:  60% [0:00:03 of 0:00:08 remaining]
Chain 3:  70% [0:00:02 of 0:00:08 remaining]
Chain 3:  80% [0:00:02 of 0:00:08 remaining]
Chain 3:  90% [0:00:01 of 0:00:08 remaining]
Chain 3: 100% [0:00:00 of 0:00:07 remaining]

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Empirical Posterior Estimates:
           Mean        SD       Naive SE       MCSE         ESS   
beta[1] 0.55579525 1.15267638 0.0095314606 0.0183053854 3965.12636
beta[2] 0.80969858 0.34968853 0.0028915682 0.0051270302 4651.90694
     s2 1.28964686 1.91794920 0.0158594880 0.0918806486  435.73866

Quantiles:
            2.5%        25.0%       50.0%      75.0%     97.5%  
beta[1] -1.79964357 -0.028876987 0.57619202 1.14748540 2.9294571
beta[2]  0.09273085  0.632011044 0.80410375 0.98559417 1.5400942
     s2  0.17672115  0.398691262 0.69776054 1.36035876 6.2323394


>>> Testing ../doc/mcmc/newmultivardist.jl

WARNING: replacing module Testing
MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:00:20 of 0:00:20 remaining]
Chain 1:  10% [0:00:05 of 0:00:06 remaining]
Chain 1:  20% [0:00:07 of 0:00:09 remaining]
Chain 1:  30% [0:00:07 of 0:00:10 remaining]
Chain 1:  40% [0:00:06 of 0:00:10 remaining]
Chain 1:  50% [0:00:05 of 0:00:10 remaining]
Chain 1:  60% [0:00:04 of 0:00:10 remaining]
Chain 1:  70% [0:00:03 of 0:00:10 remaining]
Chain 1:  80% [0:00:02 of 0:00:10 remaining]
Chain 1:  90% [0:00:01 of 0:00:10 remaining]
Chain 1: 100% [0:00:00 of 0:00:09 remaining]

Chain 2:   0% [0:00:04 of 0:00:04 remaining]
Chain 2:  10% [0:00:26 of 0:00:28 remaining]
Chain 2:  20% [0:00:15 of 0:00:19 remaining]
Chain 2:  30% [0:00:10 of 0:00:14 remaining]
Chain 2:  40% [0:00:07 of 0:00:12 remaining]
Chain 2:  50% [0:00:06 of 0:00:11 remaining]
Chain 2:  60% [0:00:04 of 0:00:11 remaining]
Chain 2:  70% [0:00:03 of 0:00:10 remaining]
Chain 2:  80% [0:00:02 of 0:00:10 remaining]
Chain 2:  90% [0:00:01 of 0:00:10 remaining]
Chain 2: 100% [0:00:00 of 0:00:10 remaining]

Chain 3:   0% [0:00:13 of 0:00:13 remaining]
Chain 3:  10% [0:00:09 of 0:00:11 remaining]
Chain 3:  20% [0:00:07 of 0:00:09 remaining]
Chain 3:  30% [0:00:07 of 0:00:10 remaining]
Chain 3:  40% [0:00:07 of 0:00:11 remaining]
Chain 3:  50% [0:00:06 of 0:00:11 remaining]
Chain 3:  60% [0:00:05 of 0:00:12 remaining]
Chain 3:  70% [0:00:04 of 0:00:12 remaining]
Chain 3:  80% [0:00:02 of 0:00:12 remaining]
Chain 3:  90% [0:00:01 of 0:00:12 remaining]
Chain 3: 100% [0:00:00 of 0:00:13 remaining]

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Empirical Posterior Estimates:
           Mean       SD       Naive SE      MCSE        ESS   
beta[1] 0.5723096 1.58471566 0.013103986 0.028576337 3075.31491
beta[2] 0.8087314 0.48297018 0.003993672 0.008639592 3125.02992
     s2 2.4091696 6.71474166 0.055524080 0.534822756  157.62999

Quantiles:
            2.5%        25.0%       50.0%      75.0%      97.5%  
beta[1] -2.493504726 -0.03617528 0.57526588 1.19388432  3.5983589
beta[2] -0.043138713  0.62261992 0.80417483 0.98644642  1.6885985
     s2  0.171595126  0.39362484 0.69577203 1.43441624 27.0877330

INFO: Mamba tests passed

>>> End of log
