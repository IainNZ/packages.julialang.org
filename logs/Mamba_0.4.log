>>> 'Pkg.add("Mamba")' log
INFO: Cloning cache of Cairo from git://github.com/JuliaLang/Cairo.jl.git
INFO: Cloning cache of Graphics from git://github.com/JuliaLang/Graphics.jl.git
INFO: Cloning cache of Mamba from git://github.com/brian-j-smith/Mamba.jl.git
INFO: Installing ArrayViews v0.6.4
INFO: Installing BinDeps v0.3.21
INFO: Installing Cairo v0.2.31
INFO: Installing Calculus v0.1.14
INFO: Installing Codecs v0.1.5
INFO: Installing ColorTypes v0.2.2
INFO: Installing Colors v0.6.4
INFO: Installing Compose v0.4.2
INFO: Installing Contour v0.1.0
INFO: Installing DataArrays v0.3.0
INFO: Installing DataFrames v0.7.2
INFO: Installing DataStructures v0.4.4
INFO: Installing Dates v0.4.4
INFO: Installing Distances v0.3.1
INFO: Installing Distributions v0.9.0
INFO: Installing Docile v0.5.23
INFO: Installing DualNumbers v0.2.2
INFO: Installing FixedPointNumbers v0.1.3
INFO: Installing FixedSizeArrays v0.1.2
INFO: Installing GZip v0.2.18
INFO: Installing Gadfly v0.4.2
INFO: Installing Graphics v0.1.3
INFO: Installing Graphs v0.6.0
INFO: Installing Grid v0.4.0
INFO: Installing Hexagons v0.0.4
INFO: Installing Iterators v0.1.9
INFO: Installing KernelDensity v0.1.2
INFO: Installing Loess v0.0.6
INFO: Installing Mamba v0.9.1
INFO: Installing Measures v0.0.2
INFO: Installing NaNMath v0.2.1
INFO: Installing Optim v0.4.5
INFO: Installing PDMats v0.4.1
INFO: Installing PositiveFactorizations v0.0.1
INFO: Installing Reexport v0.0.3
INFO: Installing SHA v0.1.2
INFO: Installing Showoff v0.0.7
INFO: Installing SortingAlgorithms v0.0.6
INFO: Installing StatsBase v0.8.1
INFO: Installing StatsFuns v0.2.2
INFO: Installing URIParser v0.1.3
INFO: Installing WoodburyMatrices v0.1.5
INFO: Building Cairo
INFO: Package database updated

>>> 'Pkg.test("Mamba")' log
Julia Version 0.4.5
Commit 2ac304d (2016-03-18 00:58 UTC)
Platform Info:
  System: Linux (x86_64-unknown-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.3
INFO: Testing Mamba
Running tests:

>>> Testing ../doc/tutorial/line.jl

digraph MambaModel {
	"y" [shape="ellipse", fillcolor="gray85", style="filled"];
	"mu" [shape="diamond", fillcolor="gray85", style="filled"];
		"mu" -> "y";
	"s2" [shape="ellipse"];
		"s2" -> "y";
	"beta" [shape="ellipse"];
		"beta" -> "mu";
	"xmat" [shape="box", fillcolor="gray85", style="filled"];
		"xmat" -> "mu";
}
MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:31:47 of 0:31:49 remaining]
Chain 1:  10% [0:00:26 of 0:00:28 remaining]
Chain 1:  20% [0:00:15 of 0:00:18 remaining]
Chain 1:  30% [0:00:11 of 0:00:15 remaining]
Chain 1:  40% [0:00:08 of 0:00:14 remaining]
Chain 1:  50% [0:00:06 of 0:00:13 remaining]
Chain 1:  60% [0:00:05 of 0:00:12 remaining]
Chain 1:  70% [0:00:03 of 0:00:11 remaining]
Chain 1:  80% [0:00:02 of 0:00:10 remaining]
Chain 1:  90% [0:00:01 of 0:00:10 remaining]
Chain 1: 100% [0:00:00 of 0:00:10 remaining]

Chain 2:   0% [0:00:08 of 0:00:08 remaining]
Chain 2:  10% [0:00:05 of 0:00:06 remaining]
Chain 2:  20% [0:00:04 of 0:00:05 remaining]
Chain 2:  30% [0:00:03 of 0:00:05 remaining]
Chain 2:  40% [0:00:03 of 0:00:05 remaining]
Chain 2:  50% [0:00:03 of 0:00:05 remaining]
Chain 2:  60% [0:00:02 of 0:00:05 remaining]
Chain 2:  70% [0:00:02 of 0:00:06 remaining]
Chain 2:  80% [0:00:01 of 0:00:06 remaining]
Chain 2:  90% [0:00:01 of 0:00:06 remaining]
Chain 2: 100% [0:00:00 of 0:00:06 remaining]

Chain 3:   0% [0:00:06 of 0:00:06 remaining]
Chain 3:  10% [0:00:06 of 0:00:06 remaining]
Chain 3:  20% [0:00:05 of 0:00:06 remaining]
Chain 3:  30% [0:00:04 of 0:00:06 remaining]
Chain 3:  40% [0:00:03 of 0:00:06 remaining]
Chain 3:  50% [0:00:03 of 0:00:06 remaining]
Chain 3:  60% [0:00:02 of 0:00:06 remaining]
Chain 3:  70% [0:00:02 of 0:00:06 remaining]
Chain 3:  80% [0:00:01 of 0:00:05 remaining]
Chain 3:  90% [0:00:01 of 0:00:05 remaining]
Chain 3: 100% [0:00:00 of 0:00:05 remaining]

MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:01:12 of 0:01:12 remaining]
Chain 1:  10% [0:00:09 of 0:00:10 remaining]
Chain 1:  20% [0:00:08 of 0:00:10 remaining]
Chain 1:  30% [0:00:07 of 0:00:09 remaining]
Chain 1:  40% [0:00:06 of 0:00:10 remaining]
Chain 1:  50% [0:00:05 of 0:00:09 remaining]
Chain 1:  60% [0:00:04 of 0:00:09 remaining]
Chain 1:  70% [0:00:03 of 0:00:09 remaining]
Chain 1:  80% [0:00:02 of 0:00:09 remaining]
Chain 1:  90% [0:00:01 of 0:00:08 remaining]
Chain 1: 100% [0:00:00 of 0:00:08 remaining]

Chain 2:   0% [0:00:08 of 0:00:08 remaining]
Chain 2:  10% [0:00:08 of 0:00:09 remaining]
Chain 2:  20% [0:00:08 of 0:00:10 remaining]
Chain 2:  30% [0:00:06 of 0:00:09 remaining]
Chain 2:  40% [0:00:05 of 0:00:09 remaining]
Chain 2:  50% [0:00:04 of 0:00:09 remaining]
Chain 2:  60% [0:00:04 of 0:00:09 remaining]
Chain 2:  70% [0:00:03 of 0:00:09 remaining]
Chain 2:  80% [0:00:02 of 0:00:09 remaining]
Chain 2:  90% [0:00:01 of 0:00:10 remaining]
Chain 2: 100% [0:00:00 of 0:00:10 remaining]

Chain 3:   0% [0:00:08 of 0:00:08 remaining]
Chain 3:  10% [0:00:06 of 0:00:07 remaining]
Chain 3:  20% [0:00:05 of 0:00:06 remaining]
Chain 3:  30% [0:00:04 of 0:00:06 remaining]
Chain 3:  40% [0:00:04 of 0:00:06 remaining]
Chain 3:  50% [0:00:03 of 0:00:06 remaining]
Chain 3:  60% [0:00:02 of 0:00:06 remaining]
Chain 3:  70% [0:00:02 of 0:00:06 remaining]
Chain 3:  80% [0:00:01 of 0:00:06 remaining]
Chain 3:  90% [0:00:01 of 0:00:06 remaining]
Chain 3: 100% [0:00:00 of 0:00:06 remaining]

MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:15:56 of 0:15:57 remaining]
Chain 1:  10% [0:00:09 of 0:00:10 remaining]
Chain 1:  20% [0:00:04 of 0:00:05 remaining]
Chain 1:  30% [0:00:03 of 0:00:04 remaining]
Chain 1:  40% [0:00:02 of 0:00:03 remaining]
Chain 1:  50% [0:00:01 of 0:00:02 remaining]
Chain 1:  60% [0:00:01 of 0:00:02 remaining]
Chain 1:  70% [0:00:01 of 0:00:02 remaining]
Chain 1:  80% [0:00:00 of 0:00:02 remaining]
Chain 1:  90% [0:00:00 of 0:00:02 remaining]
Chain 1: 100% [0:00:00 of 0:00:02 remaining]

Chain 2:   0% [0:00:01 of 0:00:01 remaining]
Chain 2:  10% [0:00:01 of 0:00:01 remaining]
Chain 2:  20% [0:00:01 of 0:00:01 remaining]
Chain 2:  30% [0:00:00 of 0:00:01 remaining]
Chain 2:  40% [0:00:00 of 0:00:01 remaining]
Chain 2:  50% [0:00:00 of 0:00:01 remaining]
Chain 2:  60% [0:00:00 of 0:00:01 remaining]
Chain 2:  70% [0:00:00 of 0:00:01 remaining]
Chain 2:  80% [0:00:00 of 0:00:01 remaining]
Chain 2:  90% [0:00:00 of 0:00:01 remaining]
Chain 2: 100% [0:00:00 of 0:00:01 remaining]

Chain 3:   0% [0:00:00 of 0:00:00 remaining]
Chain 3:  10% [0:00:01 of 0:00:01 remaining]
Chain 3:  20% [0:00:01 of 0:00:01 remaining]
Chain 3:  30% [0:00:00 of 0:00:01 remaining]
Chain 3:  40% [0:00:00 of 0:00:01 remaining]
Chain 3:  50% [0:00:00 of 0:00:01 remaining]
Chain 3:  60% [0:00:00 of 0:00:01 remaining]
Chain 3:  70% [0:00:00 of 0:00:01 remaining]
Chain 3:  80% [0:00:00 of 0:00:01 remaining]
Chain 3:  90% [0:00:00 of 0:00:01 remaining]
Chain 3: 100% [0:00:00 of 0:00:01 remaining]

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Gelman, Rubin, and Brooks Diagnostic:
              PSRF 97.5%
     beta[1] 1.009 1.010
     beta[2] 1.009 1.010
          s2 1.008 1.016
Multivariate 1.006   NaN

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Geweke Diagnostic:
First Window Fraction = 0.1
Second Window Fraction = 0.5

        Z-score p-value
beta[1]   1.237  0.2162
beta[2]  -1.568  0.1168
     s2   1.710  0.0872

        Z-score p-value
beta[1]  -1.457  0.1452
beta[2]   1.752  0.0797
     s2  -1.428  0.1534

        Z-score p-value
beta[1]   0.550  0.5824
beta[2]  -0.440  0.6597
     s2   0.583  0.5596

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Heidelberger and Welch Diagnostic:
Target Halfwidth Ratio = 0.1
Alpha = 0.05

        Burn-in Stationarity p-value    Mean     Halfwidth  Test
beta[1]     251            1  0.0680 0.57366275 0.053311283    1
beta[2]     738            1  0.0677 0.81285744 0.015404173    1
     s2     738            1  0.0700 1.00825202 0.094300432    1

        Burn-in Stationarity p-value    Mean     Halfwidth  Test
beta[1]     251            1  0.1356  0.6293320 0.065092099    0
beta[2]     251            1  0.0711  0.7934633 0.019215278    1
     s2     251            1  0.4435  1.4635400 0.588158612    0

        Burn-in Stationarity p-value    Mean     Halfwidth  Test
beta[1]     251            1  0.0515  0.5883602 0.058928034    0
beta[2]    1225            1  0.1479  0.8086080 0.018478999    1
     s2     251            1  0.6664  0.9942853 0.127959523    0

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Raftery and Lewis Diagnostic:
Quantile (q) = 0.025
Accuracy (r) = 0.005
Probability (s) = 0.95

        Thinning Burn-in    Total   Nmin Dependence Factor
beta[1]        2     267      17897 3746         4.7776295
beta[2]        2     267      17897 3746         4.7776295
     s2        2     257       8689 3746         2.3195408

        Thinning Burn-in    Total   Nmin Dependence Factor
beta[1]        4     271 2.1759×10⁴ 3746         5.8085958
beta[2]        4     275 2.8795×10⁴ 3746         7.6868660
     s2        2     257 8.3450×10³ 3746         2.2277096

        Thinning Burn-in    Total   Nmin Dependence Factor
beta[1]        2     269 2.0647×10⁴ 3746         5.5117459
beta[2]        2     263 1.4523×10⁴ 3746         3.8769354
     s2        2     255 7.8770×10³ 3746         2.1027763

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Empirical Posterior Estimates:
           Mean       SD       Naive SE       MCSE       ESS   
beta[1] 0.5971183 1.14894446 0.0095006014 0.016925598 4607.9743
beta[2] 0.8017036 0.34632566 0.0028637608 0.004793345 4875.0000
     s2 1.2203777 2.00876760 0.0166104638 0.101798287  389.3843

Quantiles:
            2.5%       25.0%       50.0%     75.0%     97.5%  
beta[1] -1.74343373 0.026573102 0.59122696 1.1878720 2.8308472
beta[2]  0.12168742 0.628297573 0.80357822 0.9719441 1.5051573
     s2  0.17091385 0.383671702 0.65371989 1.2206381 6.0313970

         95% Lower  95% Upper
beta[1] -1.75436235 2.8109571
beta[2]  0.09721501 1.4733163
     s2  0.08338409 3.8706865

           beta[1]      beta[2]        s2     
beta[1]  1.000000000 -0.905245029  0.027467317
beta[2] -0.905245029  1.000000000 -0.024489462
     s2  0.027467317 -0.024489462  1.000000000

           Lag 2       Lag 10        Lag 20       Lag 100   
beta[1] 0.24521566  -0.021411797 -0.0077424153  -0.044989417
beta[2] 0.20402485  -0.019107846  0.0033980453  -0.053869216
     s2 0.85931351   0.568056917  0.3248136852   0.024157524

           Lag 2       Lag 10        Lag 20       Lag 100   
beta[1] 0.28180489  -0.031007672    0.03930888  0.0394895028
beta[2] 0.25905976  -0.017946010    0.03613043  0.0227758214
     s2 0.92905843   0.761339226    0.58455868  0.0050215824

           Lag 2       Lag 10        Lag 20       Lag 100   
beta[1] 0.38634357 -0.0029361782  -0.032310111  0.0028806786
beta[2] 0.32822879 -0.0056670786  -0.020100663 -0.0062622517
     s2 0.68812720  0.2420402859   0.080495078 -0.0290205896

             Change Rate
     beta[1]       0.844
     beta[2]       0.844
          s2       1.000
Multivariate       1.000

MCMC Processing of 4875 Iterations x 3 Chains...

Chain 1:   0% [0:02:25 of 0:02:25 remaining]
Chain 1:  10% [0:00:03 of 0:00:03 remaining]
Chain 1:  20% [0:00:01 of 0:00:02 remaining]
Chain 1:  30% [0:00:01 of 0:00:01 remaining]
Chain 1:  40% [0:00:00 of 0:00:01 remaining]
Chain 1:  50% [0:00:00 of 0:00:01 remaining]
Chain 1:  60% [0:00:00 of 0:00:01 remaining]
Chain 1:  70% [0:00:00 of 0:00:01 remaining]
Chain 1:  80% [0:00:00 of 0:00:00 remaining]
Chain 1:  90% [0:00:00 of 0:00:00 remaining]
Chain 1: 100% [0:00:00 of 0:00:00 remaining]

Chain 2:   0% [0:03:43 of 0:03:43 remaining]
Chain 2:  10% [0:00:04 of 0:00:05 remaining]
Chain 2:  20% [0:00:02 of 0:00:02 remaining]
Chain 2:  30% [0:00:01 of 0:00:02 remaining]
Chain 2:  40% [0:00:01 of 0:00:01 remaining]
Chain 2:  50% [0:00:00 of 0:00:01 remaining]
Chain 2:  60% [0:00:00 of 0:00:01 remaining]
Chain 2:  70% [0:00:00 of 0:00:01 remaining]
Chain 2:  80% [0:00:00 of 0:00:01 remaining]
Chain 2:  90% [0:00:00 of 0:00:01 remaining]
Chain 2: 100% [0:00:00 of 0:00:01 remaining]

Chain 3:   0% [0:04:12 of 0:04:12 remaining]
Chain 3:  10% [0:00:05 of 0:00:05 remaining]
Chain 3:  20% [0:00:02 of 0:00:03 remaining]
Chain 3:  30% [0:00:01 of 0:00:02 remaining]
Chain 3:  40% [0:00:01 of 0:00:01 remaining]
Chain 3:  50% [0:00:01 of 0:00:01 remaining]
Chain 3:  60% [0:00:00 of 0:00:01 remaining]
Chain 3:  70% [0:00:00 of 0:00:01 remaining]
Chain 3:  80% [0:00:00 of 0:00:01 remaining]
Chain 3:  90% [0:00:00 of 0:00:01 remaining]
Chain 3: 100% [0:00:00 of 0:00:01 remaining]

      DIC    Effective Parameters
pD 13.828540            1.1661193
pV 22.624104            5.5639015

Iterations = 1000:5000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 2001

Empirical Posterior Estimates:
           Mean        SD      Naive SE      MCSE       ESS   
beta[1] 0.62445845 1.0285709 0.013275474 0.023818436 1864.8416
beta[2] 0.79392648 0.3096614 0.003996712 0.006516677 2001.0000

Quantiles:
            2.5%       25.0%       50.0%     75.0%     97.5%  
beta[1] -1.53050898 0.076745702 0.61120944 1.2174641 2.6906753
beta[2]  0.18846617 0.618849048 0.79323126 0.9619767 1.4502109

MCMC Simulation of 5000 Iterations x 3 Chains...

Chain 1:   0% [0:00:14 of 0:00:14 remaining]
Chain 1:  10% [0:00:02 of 0:00:03 remaining]
Chain 1:  20% [0:00:02 of 0:00:02 remaining]
Chain 1:  30% [0:00:02 of 0:00:03 remaining]
Chain 1:  40% [0:00:02 of 0:00:03 remaining]
Chain 1:  50% [0:00:02 of 0:00:03 remaining]
Chain 1:  60% [0:00:01 of 0:00:03 remaining]
Chain 1:  70% [0:00:01 of 0:00:03 remaining]
Chain 1:  80% [0:00:01 of 0:00:03 remaining]
Chain 1:  90% [0:00:00 of 0:00:04 remaining]
Chain 1: 100% [0:00:00 of 0:00:04 remaining]

Chain 2:   0% [0:00:07 of 0:00:07 remaining]
Chain 2:  10% [0:00:04 of 0:00:04 remaining]
Chain 2:  20% [0:00:02 of 0:00:03 remaining]
Chain 2:  30% [0:00:02 of 0:00:03 remaining]
Chain 2:  40% [0:00:02 of 0:00:03 remaining]
Chain 2:  50% [0:00:02 of 0:00:03 remaining]
Chain 2:  60% [0:00:01 of 0:00:03 remaining]
Chain 2:  70% [0:00:01 of 0:00:03 remaining]
Chain 2:  80% [0:00:01 of 0:00:03 remaining]
Chain 2:  90% [0:00:00 of 0:00:03 remaining]
Chain 2: 100% [0:00:00 of 0:00:03 remaining]

Chain 3:   0% [0:00:01 of 0:00:01 remaining]
Chain 3:  10% [0:00:03 of 0:00:03 remaining]
Chain 3:  20% [0:00:03 of 0:00:03 remaining]
Chain 3:  30% [0:00:02 of 0:00:03 remaining]
Chain 3:  40% [0:00:02 of 0:00:03 remaining]
Chain 3:  50% [0:00:01 of 0:00:03 remaining]
Chain 3:  60% [0:00:01 of 0:00:03 remaining]
Chain 3:  70% [0:00:01 of 0:00:03 remaining]
Chain 3:  80% [0:00:00 of 0:00:02 remaining]
Chain 3:  90% [0:00:00 of 0:00:02 remaining]
Chain 3: 100% [0:00:00 of 0:00:02 remaining]

Iterations = 252:15000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 7375

Empirical Posterior Estimates:
           Mean        SD      Naive SE       MCSE        ESS   
beta[1] 0.59655228 1.1225920 0.0075471034 0.014053505 6380.79199
beta[2] 0.80144540 0.3395731 0.0022829250 0.003954871 7372.28048
     s2 1.18366563 1.8163096 0.0122109158 0.070481708  664.08995

Quantiles:
            2.5%       25.0%       50.0%     75.0%     97.5%  
beta[1] -1.70512374 0.031582137 0.58989089 1.1783924 2.8253668
beta[2]  0.12399073 0.630638800 0.80358526 0.9703569 1.4939817
     s2  0.17075261 0.382963160 0.65372440 1.2210168 5.7449800

Object of type "Mamba.Model"
-------------------------------------------------------------------------------
y:
An unmonitored node of type "5-element Mamba.ArrayStochastic{1}"
[1.0,3.0,3.0,3.0,5.0]

Distribution:
IsoNormal(
dim: 5
μ: [3.2384475887755166,5.286627296564756,7.334807004353996,9.382986712143236,11.431166419932476]
Σ: [1.1830434911443408 0.0 0.0 0.0 0.0
 0.0 1.1830434911443408 0.0 0.0 0.0
 0.0 0.0 1.1830434911443408 0.0 0.0
 0.0 0.0 0.0 1.1830434911443408 0.0
 0.0 0.0 0.0 0.0 1.1830434911443408]
)

Function:
AST(:($(Expr(:lambda, Any[:(model::Model)], Any[Any[Any[:model,:Any,18],Any[:f,:Any,18]],Any[],0,Any[]], :(begin 
        model = (top(typeassert))(model,Mamba.Model)
        f = (anonymous function)
        return f((Mamba.getindex)(model,:mu),(Mamba.getindex)(model,:s2))
    end)))))

Source Nodes:
[:mu,:s2]

Target Nodes:
Symbol[]
-------------------------------------------------------------------------------
s2:
A monitored node of type "Mamba.ScalarStochastic"
1.1830434911443408

Distribution:
Distributions.InverseGamma(
invd: Distributions.Gamma(α=0.001, θ=1000.0)
θ: 0.001
)

Function:
AST(:($(Expr(:lambda, Any[:(model::Model)], Any[Any[Any[:model,:Any,18],Any[:f,:Any,18]],Any[],0,Any[]], :(begin 
        model = (top(typeassert))(model,Mamba.Model)
        f = (anonymous function)
        return f()
    end)))))

Source Nodes:
Symbol[]

Target Nodes:
[:y]
-------------------------------------------------------------------------------
xmat:
[1.0 1.0
 1.0 2.0
 1.0 3.0
 1.0 4.0
 1.0 5.0]
-------------------------------------------------------------------------------
beta:
A monitored node of type "2-element Mamba.ArrayStochastic{1}"
[1.1902678809862768,2.04817970778924]

Distribution:
ZeroMeanIsoNormal(
dim: 2
μ: [0.0,0.0]
Σ: [1000.0 0.0
 0.0 1000.0]
)

Function:
AST(:($(Expr(:lambda, Any[:(model::Model)], Any[Any[Any[:model,:Any,18],Any[:f,:Any,18]],Any[],0,Any[]], :(begin 
        model = (top(typeassert))(model,Mamba.Model)
        f = (anonymous function)
        return f()
    end)))))

Source Nodes:
Symbol[]

Target Nodes:
[:mu,:y]
-------------------------------------------------------------------------------
mu:
An unmonitored node of type "5-element Mamba.ArrayLogical{1}"
[3.2384475887755166,5.286627296564756,7.334807004353996,9.382986712143236,11.431166419932476]
Function:
AST(:($(Expr(:lambda, Any[:(model::Model)], Any[Any[Any[:model,:Any,18],Any[:f,:Any,18]],Any[],0,Any[]], :(begin 
        model = (top(typeassert))(model,Mamba.Model)
        f = (anonymous function)
        return f((Mamba.getindex)(model,:xmat),(Mamba.getindex)(model,:beta))
    end)))))

Source Nodes:
[:xmat,:beta]

Target Nodes:
[:y]

>>> Testing ../doc/samplers/amm.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE       MCSE       ESS   
b0 0.67372957 1.07776604 0.0152419136 0.064652461 277.89381
b1 0.77112512 0.32135034 0.0045445801 0.017982404 319.34640
s2 1.29658296 2.18979550 0.0309683849 0.139974432 244.74267

Quantiles:
       2.5%        25.0%       50.0%      75.0%     97.5%  
b0 -1.451022943 0.069490369 0.65978541 1.19631191 3.1685322
b1  0.050258268 0.611291896 0.79094107 0.94652458 1.4132212
s2  0.174653274 0.389575233 0.65132492 1.31853897 6.8337311


>>> Testing ../doc/samplers/amwg.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean       SD      Naive SE       MCSE        ESS   
b0 0.5983881 1.5898262 0.0224835377 0.162560986  95.645964
b1 0.8006155 0.4358909 0.0061644284 0.042038394 107.513597
s2 2.2710644 9.1262030 0.1290640007 0.686946293 176.495925

Quantiles:
       2.5%       25.0%      50.0%      75.0%      97.5%  
b0 -2.17471713 0.03038741 0.67912808 1.30557650  3.3496859
b1  0.03755749 0.60096712 0.77599921 0.96011511  1.7513752
s2  0.16901125 0.39945048 0.70340929 1.47274298 13.3376638


>>> Testing ../doc/samplers/bhmc.jl

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean      SD       Naive SE       MCSE         ESS   
 gamma[1] 0.5425 0.49821539 0.0049821539 0.0043794170 10000.0000
 gamma[2] 1.0000 0.00000000 0.0000000000 0.0000000000 10000.0000
 gamma[3] 0.3125 0.46353558 0.0046353558 0.0052442850  7812.5639
 gamma[4] 1.0000 0.00000000 0.0000000000 0.0000000000 10000.0000
 gamma[5] 0.7132 0.45228997 0.0045228997 0.0056726884  6357.0562
 gamma[6] 0.7342 0.44178035 0.0044178035 0.0055526497  6330.1242
 gamma[7] 1.0000 0.00000000 0.0000000000 0.0000000000 10000.0000
 gamma[8] 0.5202 0.49961677 0.0049961677 0.0030648875 10000.0000
 gamma[9] 0.4972 0.50001716 0.0050001716 0.0063230858  6253.3346
gamma[10] 0.7768 0.41641218 0.0041641218 0.0044537579  8741.6542

Quantiles:
          2.5% 25.0% 50.0% 75.0% 97.5%
 gamma[1]    0     0     1     1     1
 gamma[2]    1     1     1     1     1
 gamma[3]    0     0     0     1     1
 gamma[4]    1     1     1     1     1
 gamma[5]    0     0     1     1     1
 gamma[6]    0     0     1     1     1
 gamma[7]    1     1     1     1     1
 gamma[8]    0     0     1     1     1
 gamma[9]    0     0     0     1     1
gamma[10]    0     1     1     1     1


>>> Testing ../doc/samplers/bmc3.jl

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean       SD        Naive SE       MCSE          ESS    
 gamma[1] 0.6555 0.475228718 0.00475228718 0.0179366872   701.973771
 gamma[2] 0.9970 0.054692770 0.00054692770 0.0030000000   332.366570
 gamma[3] 0.0039 0.062331200 0.00062331200 0.0030580082   415.464277
 gamma[4] 0.9991 0.029987996 0.00029987996 0.0009000000  1110.222133
 gamma[5] 0.9809 0.136883395 0.00136883395 0.0134525691   103.535925
 gamma[6] 0.9940 0.077230800 0.00077230800 0.0042592739   328.783435
 gamma[7] 0.9999 0.010000000 0.00010000000 0.0001000000 10000.000000
 gamma[8] 0.6844 0.464777626 0.00464777626 0.0154549329   904.390896
 gamma[9] 0.0162 0.126250362 0.00126250362 0.0114192642   122.233154
gamma[10] 0.9833 0.128151287 0.00128151287 0.0117508263   118.934849

Quantiles:
          2.5% 25.0% 50.0% 75.0% 97.5%
 gamma[1]    0     0     1     1     1
 gamma[2]    1     1     1     1     1
 gamma[3]    0     0     0     0     0
 gamma[4]    1     1     1     1     1
 gamma[5]    1     1     1     1     1
 gamma[6]    1     1     1     1     1
 gamma[7]    1     1     1     1     1
 gamma[8]    0     0     1     1     1
 gamma[9]    0     0     0     0     0
gamma[10]    1     1     1     1     1


>>> Testing ../doc/samplers/bmg.jl

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean       SD        Naive SE       MCSE         ESS   
 gamma[1] 0.6668 0.471380929 0.00471380929 0.0194701948  586.14278
 gamma[2] 0.9994 0.024488772 0.00024488772 0.0006000000 1665.83325
 gamma[3] 0.0174 0.130762953 0.00130762953 0.0100811054  168.24925
 gamma[4] 0.9990 0.031608542 0.00031608542 0.0010000000  999.09991
 gamma[5] 0.9453 0.227405104 0.00227405104 0.0222966478  104.02106
 gamma[6] 0.9969 0.055594056 0.00055594056 0.0019053089  851.38487
 gamma[7] 0.9972 0.052843535 0.00052843535 0.0028000000  356.17847
 gamma[8] 0.6489 0.477338009 0.00477338009 0.0182479527  684.26408
 gamma[9] 0.0560 0.229933223 0.00229933223 0.0224625390  104.78178
gamma[10] 0.9570 0.202867236 0.00202867236 0.0196069973  107.05369

Quantiles:
          2.5% 25.0% 50.0% 75.0% 97.5%
 gamma[1]    0     0     1     1     1
 gamma[2]    1     1     1     1     1
 gamma[3]    0     0     0     0     0
 gamma[4]    1     1     1     1     1
 gamma[5]    0     1     1     1     1
 gamma[6]    1     1     1     1     1
 gamma[7]    1     1     1     1     1
 gamma[8]    0     0     1     1     1
 gamma[9]    0     0     0     0     1
gamma[10]    0     1     1     1     1


>>> Testing ../doc/samplers/hmc.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE       MCSE       ESS   
b0 0.5772432  1.18402293 0.0167446128 0.027883682 1803.1006
b1 0.8022538  0.37671405 0.0053275412 0.008170591 2125.7719
s2 1.4971811 11.28460781 0.1595884542 0.195664541 3326.2025

Quantiles:
       2.5%        25.0%       50.0%      75.0%     97.5%  
b0 -1.714610896 0.015592733 0.55534270 1.17186976 3.0307402
b1  0.092367742 0.631187676 0.80982814 0.97620155 1.4874951
s2  0.178874019 0.383680766 0.66198223 1.22565452 7.0403625

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE       MCSE       ESS   
b0 0.59496699 1.37677793 0.0194705802 0.035517823 1502.5714
b1 0.79853916 0.41130398 0.0058167167 0.009917444 1719.9916
s2 1.80970811 8.36078976 0.1182394227 0.312469964  715.9423

Quantiles:
       2.5%        25.0%      50.0%     75.0%     97.5%  
b0 -1.89540401 -0.028843232 0.6365522 1.2257351 3.0797518
b1  0.06653081  0.605516575 0.7862590 0.9763511 1.5777984
s2  0.16937961  0.390805781 0.6795564 1.3775763 8.2227653


>>> Testing ../doc/samplers/mala.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE      MCSE        ESS   
b0 0.63913008 0.96456516 0.013641011 0.109737267  77.260028
b1 0.79071667 0.29199225 0.004129394 0.030293853  92.903834
s2 1.16990867 2.21952343 0.031388801 0.159528112 193.573104

Quantiles:
       2.5%       25.0%       50.0%     75.0%     97.5%  
b0 -1.11636066 0.021295738 0.56942794 1.1958553 2.8185967
b1  0.13351555 0.622505238 0.81840154 0.9631912 1.3448359
s2  0.15082458 0.428174945 0.70535341 1.1853768 4.7815748

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean         SD       Naive SE      MCSE       ESS   
b0 0.61457782  1.09976885 0.0155530802 0.12576800  76.46497
b1 0.73181199  0.40025128 0.0056604079 0.04444227  81.10973
s2 2.93138741 13.82161613 0.1954671698 1.21062808 130.34554

Quantiles:
       2.5%       25.0%     50.0%     75.0%      97.5%  
b0 -1.29782609 0.00000000 0.4671962 1.0542319  3.6409243
b1 -0.09214223 0.57229743 0.7547751 0.9690808  1.4525190
s2  0.23962808 0.44855541 0.8566210 1.6689977 13.7557720


>>> Testing ../doc/samplers/nuts.jl

Iterations = 1001:5000
Thinning interval = 1
Chains = 1
Samples per chain = 4000

Empirical Posterior Estimates:
      Mean        SD       Naive SE      MCSE       ESS   
b0 0.59242566 1.12664340 0.0178137962 0.07262530 240.65621
b1 0.79955933 0.34124494 0.0053955563 0.02021753 284.88937
s2 1.23041170 2.28573689 0.0361406736 0.10397815 483.24600

Quantiles:
       2.5%        25.0%       50.0%     75.0%     97.5%  
b0 -1.60156914 -0.018689413 0.55936277 1.1737069 2.9302371
b1  0.09581153  0.620652115 0.81772866 0.9840868 1.4803526
s2  0.20677131  0.401460055 0.67740807 1.2526098 5.6494793


>>> Testing ../doc/samplers/rwm.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE       MCSE       ESS   
b0 0.50814446 0.95649687 0.0135269085 0.105741241  81.82355
b1 0.82845138 0.29152116 0.0041227318 0.031171455  87.46333
s2 1.09352799 1.51797233 0.0214673706 0.072343598 440.27852

Quantiles:
       2.5%       25.0%       50.0%     75.0%     97.5%  
b0 -1.40016641 -0.10053289 0.52105167 1.1664813 2.3420987
b1  0.27991615  0.63683768 0.81974517 1.0071005 1.4073847
s2  0.18553138  0.40129932 0.68770085 1.1990384 4.5481870


>>> Testing ../doc/samplers/slice.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE     MCSE        ESS   
b0 0.95121197 1.39800189 0.019770732 0.16058158  75.792124
b1 0.70804449 0.38972473 0.005511540 0.04083353  91.092384
s2 1.68897644 4.98568184 0.070508189 0.41400577 145.022818

Quantiles:
       2.5%       25.0%      50.0%     75.0%      97.5%  
b0 -1.18134932 0.22505842 0.75133123 1.3961206  5.2902465
b1 -0.24667682 0.57032710 0.75324890 0.9190770  1.3468726
s2  0.16329786 0.38360098 0.64859723 1.3052359 10.1810850

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean       SD       Naive SE       MCSE       ESS   
b0 0.5918173 0.97247832 0.0137529202 0.088171432 121.64775
b1 0.8010611 0.29639544 0.0041916646 0.023762408 155.58300
s2 1.1918742 2.91584961 0.0412363406 0.118877515 601.63183

Quantiles:
       2.5%       25.0%      50.0%      75.0%     97.5%  
b0 -1.53289453 0.06956086 0.62086125 1.12897029 2.5922503
b1  0.19746079 0.64147194 0.79976963 0.95676223 1.4487573
s2  0.16453963 0.36687076 0.61831108 1.15173238 5.9535381


>>> Testing ../doc/samplers/slicesimplex.jl

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
          Mean         SD        Naive SE        MCSE        ESS   
rho[1] 0.27754333 0.047233921 0.00047233921 0.00141592199 1112.8313
rho[2] 0.10332212 0.031263826 0.00031263826 0.00058567835 2849.4820
rho[3] 0.15258317 0.038007617 0.00038007617 0.00081859335 2155.7820
rho[4] 0.35246555 0.051090924 0.00051090924 0.00149244465 1171.9013
rho[5] 0.11408583 0.033523072 0.00033523072 0.00071863697 2176.0482

Quantiles:
           2.5%       25.0%      50.0%       75.0%      97.5%  
rho[1] 0.192874639 0.24366290 0.275752374 0.30784577 0.37691928
rho[2] 0.050788311 0.08068638 0.100682538 0.12286559 0.17351691
rho[3] 0.086277998 0.12523438 0.150148020 0.17687571 0.23401932
rho[4] 0.255446325 0.31691025 0.350797321 0.38773881 0.45478127
rho[5] 0.058278656 0.08940860 0.111306257 0.13540498 0.18591961


>>> Testing ../doc/mcmc/readcoda.jl

Iterations = 1:200
Thinning interval = 1
Chains = 1,2
Samples per chain = 200

Empirical Posterior Estimates:
         Mean       SD       Naive SE      MCSE       ESS   
alpha 3.0025394 0.53475753 0.026737877 0.018902157 200.00000
 beta 0.8013086 0.39267477 0.019633739 0.030895834 161.53482
sigma 1.0821777 0.94869150 0.047434575 0.061191837 200.00000

Quantiles:
         2.5%      25.0%   50.0%   75.0%     97.5%  
alpha  1.8322542 2.751095 3.0257 3.2709700 3.9511365
 beta -0.0125375 0.599750 0.8065 1.0079525 1.5292802
sigma  0.4329000 0.625000 0.8360 1.2378125 2.8597185


>>> Testing ../doc/mcmc/newunivardist.jl

MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:00:31 of 0:00:31 remaining]
Chain 1:  10% [0:00:14 of 0:00:15 remaining]
Chain 1:  20% [0:00:12 of 0:00:15 remaining]
Chain 1:  30% [0:00:11 of 0:00:16 remaining]
Chain 1:  40% [0:00:09 of 0:00:15 remaining]
Chain 1:  50% [0:00:08 of 0:00:16 remaining]
Chain 1:  60% [0:00:06 of 0:00:15 remaining]
Chain 1:  70% [0:00:05 of 0:00:15 remaining]
Chain 1:  80% [0:00:03 of 0:00:15 remaining]
Chain 1:  90% [0:00:02 of 0:00:16 remaining]
Chain 1: 100% [0:00:00 of 0:00:15 remaining]

Chain 2:   0% [0:00:24 of 0:00:24 remaining]
Chain 2:  10% [0:00:19 of 0:00:21 remaining]
Chain 2:  20% [0:00:16 of 0:00:20 remaining]
Chain 2:  30% [0:00:12 of 0:00:18 remaining]
Chain 2:  40% [0:00:11 of 0:00:18 remaining]
Chain 2:  50% [0:00:09 of 0:00:18 remaining]
Chain 2:  60% [0:00:07 of 0:00:18 remaining]
Chain 2:  70% [0:00:06 of 0:00:19 remaining]
Chain 2:  80% [0:00:04 of 0:00:18 remaining]
Chain 2:  90% [0:00:02 of 0:00:18 remaining]
Chain 2: 100% [0:00:00 of 0:00:18 remaining]

Chain 3:   0% [0:00:13 of 0:00:13 remaining]
Chain 3:  10% [0:00:17 of 0:00:19 remaining]
Chain 3:  20% [0:00:14 of 0:00:17 remaining]
Chain 3:  30% [0:00:11 of 0:00:16 remaining]
Chain 3:  40% [0:00:10 of 0:00:16 remaining]
Chain 3:  50% [0:00:08 of 0:00:16 remaining]
Chain 3:  60% [0:00:07 of 0:00:16 remaining]
Chain 3:  70% [0:00:05 of 0:00:17 remaining]
Chain 3:  80% [0:00:03 of 0:00:16 remaining]
Chain 3:  90% [0:00:02 of 0:00:16 remaining]
Chain 3: 100% [0:00:00 of 0:00:15 remaining]

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Empirical Posterior Estimates:
           Mean        SD       Naive SE       MCSE       ESS   
beta[1] 0.58240287 1.14230234 0.0094456779 0.018305636 3893.9689
beta[2] 0.80352012 0.34248004 0.0028319614 0.005181105 4369.4396
     s2 1.21355573 1.77421793 0.0146709766 0.071581268  614.3490

Quantiles:
            2.5%       25.0%       50.0%     75.0%     97.5%  
beta[1] -1.71534530 0.023088344 0.58077083 1.1524530 2.8274529
beta[2]  0.12368846 0.630518397 0.80241394 0.9732482 1.5135523
     s2  0.16868782 0.385631053 0.65901954 1.2756509 6.1752281


>>> Testing ../doc/mcmc/newmultivardist.jl

WARNING: replacing module Testing
MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:00:28 of 0:00:29 remaining]
Chain 1:  10% [0:00:09 of 0:00:10 remaining]
Chain 1:  20% [0:00:07 of 0:00:09 remaining]
Chain 1:  30% [0:00:06 of 0:00:08 remaining]
Chain 1:  40% [0:00:05 of 0:00:08 remaining]
Chain 1:  50% [0:00:04 of 0:00:08 remaining]
Chain 1:  60% [0:00:03 of 0:00:08 remaining]
Chain 1:  70% [0:00:02 of 0:00:08 remaining]
Chain 1:  80% [0:00:02 of 0:00:08 remaining]
Chain 1:  90% [0:00:01 of 0:00:08 remaining]
Chain 1: 100% [0:00:00 of 0:00:09 remaining]

Chain 2:   0% [0:00:16 of 0:00:16 remaining]
Chain 2:  10% [0:00:09 of 0:00:10 remaining]
Chain 2:  20% [0:00:08 of 0:00:09 remaining]
Chain 2:  30% [0:00:07 of 0:00:09 remaining]
Chain 2:  40% [0:00:05 of 0:00:09 remaining]
Chain 2:  50% [0:00:04 of 0:00:09 remaining]
Chain 2:  60% [0:00:04 of 0:00:09 remaining]
Chain 2:  70% [0:00:03 of 0:00:09 remaining]
Chain 2:  80% [0:00:02 of 0:00:09 remaining]
Chain 2:  90% [0:00:01 of 0:00:08 remaining]
Chain 2: 100% [0:00:00 of 0:00:08 remaining]

Chain 3:   0% [0:00:11 of 0:00:11 remaining]
Chain 3:  10% [0:00:03 of 0:00:04 remaining]
Chain 3:  20% [0:00:03 of 0:00:03 remaining]
Chain 3:  30% [0:00:02 of 0:00:03 remaining]
Chain 3:  40% [0:00:02 of 0:00:03 remaining]
Chain 3:  50% [0:00:02 of 0:00:03 remaining]
Chain 3:  60% [0:00:01 of 0:00:03 remaining]
Chain 3:  70% [0:00:01 of 0:00:04 remaining]
Chain 3:  80% [0:00:01 of 0:00:04 remaining]
Chain 3:  90% [0:00:00 of 0:00:04 remaining]
Chain 3: 100% [0:00:00 of 0:00:04 remaining]

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Empirical Posterior Estimates:
           Mean       SD       Naive SE       MCSE         ESS   
beta[1] 0.5788931 1.11232796 0.0091978202 0.0190480893 3410.06668
beta[2] 0.8052916 0.33546852 0.0027739833 0.0055032179 3715.95238
     s2 1.1638187 1.61102871 0.0133215679 0.0632533846  648.69272

Quantiles:
            2.5%       25.0%       50.0%     75.0%     97.5%  
beta[1] -1.71161012 0.030043945 0.58723102 1.1464501 2.8420839
beta[2]  0.12334042 0.628821665 0.80501158 0.9749143 1.4853405
     s2  0.17208834 0.383853394 0.66064186 1.2465013 5.5621934

INFO: Mamba tests passed

>>> End of log
